{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObFelmr0CV4jABpIUQKZJh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@theStump/how-to-beat-a-12-year-old-girl-in-battleship-using-transformers-with-reinforcement-learning-b506f7bea470)"
      ],
      "metadata": {
        "id": "MknMaRqmb9mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "\n",
        "def place_ships(board_height:int=10,board_width:int=10,ship_sizes:List[int]=[2,3,3,4,5]) -> np.ndarray:\n",
        "    \"\"\" Return random ship positions.\"\"\"\n",
        "    board = np.zeros(shape=(board_width,board_height), dtype=np.float32)\n",
        "    board_size = board_width * board_height\n",
        "    def can_place_ship(x, y, length, direction):\n",
        "        \"\"\"Check if a ship can be placed at (x, y) in a given direction without overlapping.\"\"\"\n",
        "        if direction == \"H\":  # Horizontal\n",
        "            if y + length > board_height:\n",
        "                return False\n",
        "            return all(board[x, y+i] == 0 for i in range(length))\n",
        "        else:  # Vertical\n",
        "            if x + length > board_width:\n",
        "                return False\n",
        "            return all(board[x+i, y] == 0 for i in range(length))\n",
        "\n",
        "    def place_ship(x, y, length, direction):\n",
        "        \"\"\"Place a ship at (x, y) in a given direction.\"\"\"\n",
        "        for i in range(length):\n",
        "            if direction == \"H\":\n",
        "                board[x, y+i] = 1  # Mark ship presence\n",
        "            else:\n",
        "                board[x+i, y] = 1\n",
        "\n",
        "    for ship_size in ship_sizes:\n",
        "        placed = False\n",
        "        while not placed:\n",
        "            x, y = np.random.randint(0, board_width-1),np.random.randint(0, board_height-1)\n",
        "            direction = np.random.choice([\"H\", \"V\"])  # Horizontal or Vertical\n",
        "\n",
        "            if can_place_ship(x, y, ship_size, direction):\n",
        "                place_ship(x, y, ship_size, direction)\n",
        "                placed = True\n",
        "    return np.reshape(board, (1, board_size))\n",
        "\n",
        "def print_board(board: npt.NDArray, predicted_board: npt.NDArray = None):\n",
        "    \"\"\"Print the board with proper alignment.\"\"\"\n",
        "\n",
        "    cols = board.shape[1]  # Number of columns\n",
        "    col_width = 1  # Space for each number\n",
        "    separator = \" | \"  # Column separator\n",
        "\n",
        "    # Header row\n",
        "    column_numbers = \"     \" + separator.join(f\"{chr(65+i):{col_width}}\" for i in range(cols))\n",
        "    top_border = \"  \" + \"-\" * (cols * (col_width + 3) - 1)\n",
        "\n",
        "    if predicted_board is not None:\n",
        "        column_numbers += \"           \" + separator.join(f\"{chr(65+i):{col_width}}\" for i in range(predicted_board.shape[1]))\n",
        "        top_border += \"      \" + \"-\" * (predicted_board.shape[1] * (col_width + 3) - 1)\n",
        "    print(\"Current Board \\t\\t\\t\\t\\t Predicted Board\")\n",
        "    print(column_numbers)\n",
        "    print(top_border)\n",
        "\n",
        "    # Print board row by row\n",
        "    for i, row in enumerate(board):\n",
        "        row_str = f\"{i+1:{col_width+1}} | \" + separator.join(f\"{int(cell):{col_width}}\" for cell in row) + \" |\"\n",
        "        if predicted_board is not None:\n",
        "            row_p = predicted_board[i, :]\n",
        "            row_str += \"    \" + f\"{i+1:{col_width+1}} | \" + separator.join(f\"{int(cell):{col_width}}\" for cell in row_p) + \" |\"\n",
        "        print(row_str)\n"
      ],
      "metadata": {
        "id": "-vXoo2UZccQb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        # nn.init.normal_(self.encoder_embedding.weight, mean=0, std=0.1)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model,padding_idx=0)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc_decoder = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def encoder(self, src: torch.Tensor, src_mask: torch.Tensor = None):\n",
        "        \"\"\"Encoder only model\n",
        "\n",
        "        Args:\n",
        "            src (torch.Tensor): Input current map as a Tensor [batch, boardheight*boardwidth]\n",
        "            src_mask (torch.Tensor, optional): Mask for the input. Defaults to None.\n",
        "        Returns:\n",
        "            Tensor: Probabilities shaped [batch_size, boardheight*boardwidth]\n",
        "        \"\"\"\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "        return enc_output\n",
        "\n",
        "    def decoder(self, enc_output,tgt, src_mask=None, tgt_mask=None):\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "        output = self.fc_decoder(dec_output)\n",
        "        return output\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "\n",
        "    def generate_random_mask(self, src, tgt, p:float=0.15):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 1).unsqueeze(1).unsqueeze(3)\n",
        "\n",
        "        # Apply random masking\n",
        "        random_src_mask = (torch.rand_like(src_mask.float()) > p).bool()\n",
        "        random_tgt_mask = (torch.rand_like(tgt_mask.float()) > p).bool()\n",
        "\n",
        "        src_mask = src_mask & random_src_mask\n",
        "        tgt_mask = tgt_mask & random_tgt_mask\n",
        "\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length,device=tgt.device), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = None; tgt_mask = None\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc_decoder(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "so_-weXIcipc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Board values: -1 for no guesses, 0 bomb, 1 for hit\n",
        "'''\n",
        "\n",
        "from typing import List, Tuple\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch, os\n",
        "from torch import Tensor\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os.path as osp\n",
        "from tqdm import trange,tqdm\n",
        "import numpy.typing as npt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "SHIP_SIZES = [2,3,3,4,5]\n",
        "board_height = 10\n",
        "board_width = 10\n",
        "\n",
        "\n",
        "def generate_game_data(nboards:int,board_height:int,board_width:int,ship_sizes:List[int]) -> Tuple[npt.NDArray,npt.NDArray]:\n",
        "    \"\"\"Generates dummy game data for training\n",
        "\n",
        "    Args:\n",
        "        nboards (int): number boards to generate\n",
        "        board_height (int): board height in units\n",
        "        board_width (int): board width in units\n",
        "        ship_sizes (List[int]): Array of ship sizes e.g. [2,3,3,4,5]\n",
        "        src_blank (float): percent of source board to blank out\n",
        "\n",
        "    Returns:\n",
        "        Tuple[npt.NDArray,npt.NDArray]: source, target\n",
        "    \"\"\"\n",
        "    percent_of_src_to_generate = 0.10\n",
        "    number_of_guesses = int(board_height * board_width*(1-percent_of_src_to_generate))\n",
        "    src_board = np.zeros((nboards*number_of_guesses,board_height*board_width))\n",
        "    tgt_board = np.zeros((nboards*number_of_guesses,board_height*board_width))\n",
        "    for indx in trange(nboards):\n",
        "        ship_positions = place_ships(board_height,board_width,ship_sizes)\n",
        "        ship_position_indices = np.where(ship_positions == 1)[1]\n",
        "        bomb_locations = np.arange(board_height*board_width)\n",
        "        for guess in range(number_of_guesses):\n",
        "            tgt_board[indx*number_of_guesses+guess,:] = 2*(ship_positions  == 1) + 1*(ship_positions == 0)\n",
        "\n",
        "        for p in range(board_height*board_width-number_of_guesses): # Lets guess 15 % of the board before we begin training\n",
        "            bomb_index = np.random.choice(bomb_locations)\n",
        "            src_board[indx*number_of_guesses,bomb_index] = 2 * (bomb_index in ship_position_indices) + 1 * (bomb_index not in ship_position_indices)\n",
        "            bomb_locations = np.delete(bomb_locations, np.where(bomb_locations == bomb_index))\n",
        "\n",
        "        for guess in range(1,number_of_guesses):\n",
        "            src_board[indx*number_of_guesses+guess,:] = src_board[indx*number_of_guesses+guess-1,:]\n",
        "            bomb_index = np.random.choice(bomb_locations)\n",
        "            src_board[indx*number_of_guesses+guess,bomb_index] = 2 * (bomb_index in ship_position_indices) + 1 * (bomb_index not in ship_position_indices)\n",
        "            bomb_locations = np.delete(bomb_locations, np.where(bomb_locations == bomb_index))\n",
        "\n",
        "    return src_board,tgt_board\n",
        "\n",
        "def generate_square_subsequent_mask(size):\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1)  # Upper triangular mask\n",
        "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "    return mask\n",
        "\n",
        "def apply_random_mask(tgt: torch.Tensor, mask_token: int = 0, mask_prob: float = 0.15):\n",
        "    masked_tgt = tgt.clone()\n",
        "    labels = tgt.clone()\n",
        "\n",
        "    # Create random mask\n",
        "    mask = torch.rand(tgt.shape, device=tgt.device) < mask_prob\n",
        "\n",
        "    # Replace input with mask token\n",
        "    masked_tgt[mask] = mask_token\n",
        "\n",
        "    # Optionally, ignore loss on unmasked tokens using ignore_index\n",
        "    loss_mask = mask  # use this to mask the loss later\n",
        "\n",
        "    return masked_tgt, labels, loss_mask\n",
        "\n",
        "def generate_games(ngames:int=2000,board_height:int=10,board_width:int=10,ship_sizes:List[int]=SHIP_SIZES):\n",
        "    \"\"\"Generate Games\n",
        "\n",
        "    Args:\n",
        "        ngames (int, optional): number of games to generate. Defaults to 2000.\n",
        "        board_height (int, optional): board height. Defaults to 10.\n",
        "        board_width (int, optional): board width. Defaults to 10.\n",
        "        ship_sizes (List[int], optional): ship sizes to use. Defaults to SHIP_SIZES.\n",
        "    \"\"\"\n",
        "    print(\"Generating Games to play\")\n",
        "    src,tgt = generate_game_data(ngames,board_height,board_width,SHIP_SIZES)\n",
        "\n",
        "    os.makedirs('data',exist_ok=True)\n",
        "    data = {'src':src,'tgt':tgt}\n",
        "    pickle.dump(data,open('data/training_data.pickle','wb'))\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    path = 'data'\n",
        "    files = [\n",
        "        os.path.join(path, f)\n",
        "        for f in os.listdir(path)\n",
        "        if f.endswith('.pth') and os.path.isfile(os.path.join(path, f))\n",
        "    ]\n",
        "    filename = max(files, key=os.path.getmtime)\n",
        "\n",
        "    data = torch.load(filename,map_location=device)\n",
        "\n",
        "    model = Transformer(src_vocab_size=data['model']['src_vocab_size'],\n",
        "                        tgt_vocab_size=data['model']['tgt_vocab_size'],\n",
        "                        d_model=data['model']['d_model'],\n",
        "                        num_heads=data['model']['num_heads'],\n",
        "                        num_layers=data['model']['num_layers'],\n",
        "                        d_ff=data['model']['d_ff'],\n",
        "                        max_seq_length=data['model']['max_seq_length'],\n",
        "                        dropout=data['model']['dropout']).to(device)\n",
        "\n",
        "    model.load_state_dict(data['model']['state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "    optimizer.load_state_dict(data['optimizer'])\n",
        "    try:\n",
        "        epochs = data['model']['epochs']\n",
        "    except:\n",
        "        epochs = 0\n",
        "    print(f\"Loaded model with {epochs} epochs\")\n",
        "    return model,optimizer,epochs,data\n",
        "\n",
        "def train(resume_training:bool=False,save_every_n_epoch:int=10,epochs:int=100):\n",
        "    \"\"\"This function will train the model using the data generated by generate_game_data.\n",
        "\n",
        "    Args:\n",
        "        resume_training (bool, optional): Resume training. Defaults to False.\n",
        "        save_every_n_epoch (int, optional): Save every n epochs. Defaults to 10.\n",
        "    \"\"\"\n",
        "    src_vocab_size = 3\n",
        "    tgt_vocab_size = 3 # 0, 1, 2\n",
        "    d_model = 512\n",
        "    num_heads = 4\n",
        "    num_layers = 4\n",
        "    d_ff = 2048\n",
        "    max_seq_length = board_height*board_width\n",
        "    dropout = 0.1\n",
        "    # Instantiate model\n",
        "    model = Transformer(src_vocab_size=src_vocab_size,\n",
        "                        tgt_vocab_size=tgt_vocab_size,\n",
        "                        d_model=d_model, num_heads=num_heads, num_layers=num_layers,\n",
        "                        d_ff=d_ff,\n",
        "                        max_seq_length=max_seq_length, dropout=dropout).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "    if (not osp.exists(\"data/training_data.pickle\")):\n",
        "        generate_games(ngames=20000,board_height=board_height,board_width=board_width,ship_sizes=SHIP_SIZES)\n",
        "\n",
        "    data = pickle.load(open('data/training_data.pickle','rb'))\n",
        "\n",
        "    if resume_training:\n",
        "        model,optimizer,current_epochs,_ = load_model()\n",
        "    else:\n",
        "        current_epochs = 0\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    scaler = GradScaler(device=device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "\n",
        "\n",
        "    def train_loop(src:npt.NDArray,tgt:npt.NDArray):\n",
        "        # Train the model\n",
        "        src_train, src_test, tgt_train, tgt_test = train_test_split(src, tgt, test_size=0.3,shuffle=True)\n",
        "        src_train_tensor = torch.tensor(src_train, dtype=torch.long)\n",
        "        tgt_train_tensor = torch.tensor(tgt_train, dtype=torch.long)\n",
        "        src_test_tensor = torch.tensor(src_test, dtype=torch.long)\n",
        "        tgt_test_tensor = torch.tensor(tgt_test, dtype=torch.long)\n",
        "\n",
        "        train_dataset = TensorDataset(src_train_tensor, tgt_train_tensor)       # Create a dataset\n",
        "        test_dataset = TensorDataset(src_test_tensor, tgt_test_tensor)       # Create a dataset\n",
        "\n",
        "        batch_size = 128\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Calculate class weights (adjust manually if needed)\n",
        "        all_targets = tgt_train_tensor.view(-1)\n",
        "        class_counts = torch.bincount(all_targets, minlength=3).float()\n",
        "        class_weights = 1.0 / (class_counts + 1e-6)\n",
        "        class_weights = class_weights / class_weights.sum()\n",
        "        class_weights = class_weights.to(device)\n",
        "        criterion_train = nn.CrossEntropyLoss(weight=class_weights,ignore_index=0)\n",
        "\n",
        "        all_targets = tgt_test_tensor.view(-1)\n",
        "        class_counts = torch.bincount(all_targets, minlength=3).float()\n",
        "        class_weights = 1.0 / (class_counts + 1e-6)\n",
        "        class_weights = class_weights / class_weights.sum()\n",
        "        class_weights = class_weights.to(device)\n",
        "        criterion_val = nn.CrossEntropyLoss(weight=class_weights,ignore_index=0)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            pbar = tqdm(train_loader)\n",
        "            for batch in pbar:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                src_batch, tgt_batch = batch\n",
        "                src_batch = src_batch.to(device)\n",
        "                tgt_batch = tgt_batch.to(device)\n",
        "\n",
        "                guessed_mask = (tgt_batch != 0)\n",
        "                random_mask = (torch.rand_like(tgt_batch.float()) > 0.9).to(device)\n",
        "                final_mask = guessed_mask & random_mask\n",
        "                tgt_batch_masked = tgt_batch.clone()\n",
        "                tgt_batch_masked[~final_mask] = 0\n",
        "                with autocast(device_type='cuda', dtype=torch.float16):\n",
        "                    output = model(src_batch, tgt_batch_masked)\n",
        "                    loss = criterion_train(output.view(-1, tgt_vocab_size), tgt_batch.view(-1))\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                output_tokens = output.argmax(dim=-1)\n",
        "                hits = torch.sum(output_tokens == 2).detach().cpu()\n",
        "                matches = torch.sum(output_tokens == tgt_batch).detach().cpu()\n",
        "                # print(torch.sum(matches))\n",
        "                pbar.set_description(f\"Epoch: {epoch+current_epochs:d} Train Loss: {loss.item():0.2e} Hits match {hits/batch_size:0.2f} Matches {matches/batch_size:0.2f}\")\n",
        "\n",
        "\n",
        "            pbar = tqdm(test_loader)\n",
        "            total_val_loss = 0; num_batches = 0\n",
        "            model.eval()\n",
        "            for batch in pbar:\n",
        "                src_batch, tgt_batch = batch\n",
        "                src_batch = src_batch.to(device)\n",
        "                tgt_batch = tgt_batch.to(device)\n",
        "\n",
        "                output = model(src_batch, tgt_batch)\n",
        "                val_loss = criterion_val(output.view(-1, tgt_vocab_size), tgt_batch.view(-1))\n",
        "\n",
        "                pred_classes = output.argmax(dim=-1)\n",
        "                # print(src_batch[0,:])\n",
        "                # print(pred_classes[0,:])\n",
        "\n",
        "                total_val_loss += val_loss.item()\n",
        "                num_batches += 1\n",
        "                pbar.set_description(f\"Epoch: {epoch+current_epochs:d} Train Loss: {loss.item():0.2e} Val Loss: {val_loss.item():0.2e}\")\n",
        "            average_val_loss = total_val_loss / num_batches  # Compute average validation loss\n",
        "            pbar.set_description(f\"Epoch: {epoch+current_epochs:d} Train Loss: {loss.item():0.2e} Val Loss: {average_val_loss:0.2e}\")\n",
        "            scheduler.step()\n",
        "            if (epoch % save_every_n_epoch == 0) or (epoch == epochs-1):\n",
        "                # Save the model\n",
        "                data = dict()\n",
        "                data['model'] = {\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'src_vocab_size': src_vocab_size,\n",
        "                    'tgt_vocab_size': tgt_vocab_size,\n",
        "                    'd_model': d_model,\n",
        "                    'num_heads': num_heads,\n",
        "                    'num_layers': num_layers,\n",
        "                    'd_ff': d_ff,\n",
        "                    'max_seq_length': max_seq_length,\n",
        "                    'dropout': dropout,\n",
        "                    'epochs':epoch+current_epochs\n",
        "                }\n",
        "                data['optimizer'] = optimizer.state_dict()\n",
        "                torch.save(data, f\"data/trained_model-{epoch+current_epochs}.pth\")\n",
        "                if not resume_training:\n",
        "                    torch.save(data, \"data/trained_model.bak.pth\")\n",
        "                print(f\"Saved model at epoch {epoch+current_epochs}\")\n",
        "\n",
        "    model.to(device)\n",
        "    src = data['src']\n",
        "    tgt = data['tgt']\n",
        "\n",
        "    print(\"Train Loop\")\n",
        "    train_loop(src,tgt)\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "    # generate_games(10000,board_height,board_width,SHIP_SIZES)\n",
        "    train(resume_training=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAA0uCELcSob",
        "outputId": "4d5007c3-aede-4a1d-8381-bd91dcdd0ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Games to play\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 1467/20000 [00:11<01:56, 158.41it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJsf62FrcS8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}