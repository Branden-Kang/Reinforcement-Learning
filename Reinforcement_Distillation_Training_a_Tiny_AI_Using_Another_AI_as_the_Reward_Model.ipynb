{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOX/sAIoRLWwY6u5HDvtQu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/the-ai-intelligence-index/reinforcement-distillation-training-a-tiny-ai-using-another-ai-as-the-reward-model-87b6f47210fa)"
      ],
      "metadata": {
        "id": "WnACMSbhw7y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr-b0ZY3xUFl",
        "outputId": "4a3dffcc-99f8-4f31-ca8b-312a43b2db66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ollama\n",
            "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: ollama\n",
            "Successfully installed ollama-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import ollama  # pip install ollama\n",
        "\n",
        "# ----------------------------\n",
        "# Fixed answer choices (policy output space)\n",
        "# ----------------------------\n",
        "ANSWERS = [\n",
        "    \"yes\",\n",
        "    \"no\",\n",
        "    \"maybe\",\n",
        "    \"I don't know\",\n",
        "    \"Paris\",\n",
        "    \"Mumbai\",\n",
        "    \"joke: why did the chicken cross the road?\",\n",
        "    \"greeting: hello!\",\n",
        "    \"bye!\",\n",
        "    \"thank you!\"\n",
        "]\n",
        "\n",
        "ANSWER_SIZE = len(ANSWERS)\n",
        "VOCAB_SIZE = 2000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_PATH = \"response_policy_ollama_nocache.pt\"\n",
        "OLLAMA_MODEL_NAME = \"llama3.2:1b\"  # or whatever local model you have\n",
        "\n",
        "# ----------------------------\n",
        "# Simple training dataset\n",
        "# (question, ground_truth_answer)\n",
        "# ----------------------------\n",
        "TRAIN_QA = [\n",
        "    (\"what is the capital of france?\", \"Paris\"),\n",
        "    (\"what is the capital of india?\", \"New Delhi\"),\n",
        "    (\"say hello\", \"greeting: hello!\"),\n",
        "    (\"tell me a joke\", \"joke: why did the chicken cross the road?\"),\n",
        "    (\"say goodbye\", \"bye!\"),\n",
        "]\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Policy Network\n",
        "# ----------------------------\n",
        "class ResponsePolicy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(VOCAB_SIZE, 32)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, ANSWER_SIZE)\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens: [batch, seq_len]\n",
        "        e = self.embed(tokens)       # [batch, seq_len, emb]\n",
        "        x = torch.mean(e, dim=1)     # [batch, emb]\n",
        "        logits = self.fc(x)          # [batch, ANSWER_SIZE]\n",
        "        return logits\n",
        "\n",
        "\n",
        "policy = ResponsePolicy().to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
        "ce_loss_fn = nn.CrossEntropyLoss()   # for strong \"this is correct\" updates\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Tokenizer\n",
        "# ----------------------------\n",
        "def tokenize(text: str) -> torch.Tensor:\n",
        "    tokens = text.lower().split()\n",
        "    ids = [hash(w) % VOCAB_SIZE for w in tokens]\n",
        "    return torch.tensor([ids], dtype=torch.long)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Reward model via Ollama\n",
        "# ----------------------------\n",
        "def get_llm_reward(question: str, agent_answer: str, ground_truth: str) -> int:\n",
        "    \"\"\"\n",
        "    Ask Ollama to say if the agent_answer is correct (semantically)\n",
        "    wrt the ground_truth for this question.\n",
        "    Returns +1 or -1.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are a strict evaluator.\n",
        "\n",
        "Task:\n",
        "- You are given:\n",
        "  - A user question.\n",
        "  - An agent's answer.\n",
        "  - The ground-truth correct answer.\n",
        "\n",
        "Rules:\n",
        "- If the agent's answer is semantically correct and matches the ground-truth meaning,\n",
        "  respond with exactly: +1\n",
        "- Otherwise, respond with exactly: -1\n",
        "\n",
        "No explanation, no extra text. Only output +1 or -1.\n",
        "\n",
        "Question: {question}\n",
        "Agent answer: {agent_answer}\n",
        "Ground truth answer: {ground_truth}\n",
        "\"\"\"\n",
        "\n",
        "    resp = ollama.chat(\n",
        "        model=OLLAMA_MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "        ],\n",
        "    )\n",
        "    content = resp[\"message\"][\"content\"].strip()\n",
        "    if \"+1\" in content:\n",
        "        return 1\n",
        "    elif \"-1\" in content:\n",
        "        return -1\n",
        "    else:\n",
        "        # fallback if the model misbehaves\n",
        "        return -1\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training step: RL + (optional) extra supervised push\n",
        "# ----------------------------\n",
        "def train_step(question: str, ground_truth: str):\n",
        "    # 1) forward pass\n",
        "    tokens = tokenize(question).to(device)\n",
        "    logits = policy(tokens)          # [1, ANSWER_SIZE]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    dist = Categorical(probs)\n",
        "    idx = dist.sample()              # [1]\n",
        "    action_idx = idx.item()\n",
        "    agent_answer = ANSWERS[action_idx]\n",
        "    log_prob = dist.log_prob(idx)    # scalar\n",
        "\n",
        "    # 2) get reward from Ollama\n",
        "    reward = get_llm_reward(question, agent_answer, ground_truth)\n",
        "\n",
        "    # 3) build loss\n",
        "    #    RL part (REINFORCE): encourage or discourage this sampled action\n",
        "    rl_loss = -log_prob * reward     # reward=+1 => push up log_prob; reward=-1 => push down\n",
        "\n",
        "    loss = rl_loss\n",
        "\n",
        "    # 4) If reward is positive, treat this as \"this is correct\"\n",
        "    #    and add a stronger supervised term to really lock it in.\n",
        "    if reward > 0:\n",
        "        target = torch.tensor([action_idx], dtype=torch.long, device=device)\n",
        "        ce_loss = ce_loss_fn(logits, target)   # make this action the top logit\n",
        "        # weight it more strongly than RL if you want:\n",
        "        loss = loss + 2.0 * ce_loss            # 2.0 is a hyperparameter\n",
        "\n",
        "    # 5) optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return agent_answer, reward, loss.item()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Save / load\n",
        "# ----------------------------\n",
        "def save_model(path=MODEL_PATH):\n",
        "    torch.save(policy.state_dict(), path)\n",
        "    print(f\"[+] Model saved to {path}\")\n",
        "\n",
        "\n",
        "def load_model_if_exists(path=MODEL_PATH):\n",
        "    if os.path.exists(path):\n",
        "        policy.load_state_dict(torch.load(path, map_location=device))\n",
        "        policy.train()\n",
        "        print(f\"[+] Loaded existing model from {path}\")\n",
        "    else:\n",
        "        print(\"[!] No existing model found, starting fresh.\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training loop\n",
        "# ----------------------------\n",
        "def train_with_ollama(num_steps: int = 200):\n",
        "    load_model_if_exists()\n",
        "\n",
        "    running_reward = 0.0\n",
        "    best_running_reward = -999.0\n",
        "\n",
        "    for step in range(1, num_steps + 1):\n",
        "        question, ground_truth = random.choice(TRAIN_QA)\n",
        "\n",
        "        agent_answer, reward, loss = train_step(question, ground_truth)\n",
        "\n",
        "        running_reward = 0.95 * running_reward + 0.05 * reward\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(\n",
        "                f\"Step {step:4d} | Q: {question} | agent: {agent_answer} | gt: {ground_truth} \"\n",
        "                f\"| reward={reward:+d} | running_reward={running_reward:+.3f} | loss={loss:.4f}\"\n",
        "            )\n",
        "\n",
        "        # Simple save heuristic: whenever we significantly improve running_reward\n",
        "        if running_reward > best_running_reward + 0.05:\n",
        "            best_running_reward = running_reward\n",
        "            save_model()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation: greedy answers after training\n",
        "# ----------------------------\n",
        "def eval_greedy():\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(\"No trained model found, train first.\")\n",
        "        return\n",
        "\n",
        "    policy.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "    policy.eval()\n",
        "    print(\"\\n[Eval] Greedy responses after training:\\n\")\n",
        "\n",
        "    for question, ground_truth in TRAIN_QA:\n",
        "        tokens = tokenize(question).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = policy(tokens)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx = torch.argmax(probs, dim=-1).item()\n",
        "        answer = ANSWERS[idx]\n",
        "        print(f\"Q: {question}\\n â†’ Agent: {answer}   (GT: {ground_truth})\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Training RL agent with Ollama as reward (no explicit memory, but strong +1 updates)...\")\n",
        "    train_with_ollama(num_steps=200)\n",
        "\n",
        "    print(\"\\nDone training. Running eval...\")\n",
        "    eval_greedy()"
      ],
      "metadata": {
        "id": "jclfGHM2xS4h"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}